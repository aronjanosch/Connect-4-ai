#!/usr/bin/env python
from .alpha_zero_args import AlphaZeroArgs
from .neural_net import Connect4Network, AlphaLoss, BoardData
import os
import pickle
import datetime
import numpy as np
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm_
import matplotlib.pyplot as plt

from . import util
from .timer import timer


@timer
def load_state(net: Connect4Network, optimizer: optim.Optimizer, scheduler, args: AlphaZeroArgs, iteration: np.int, new_optim_state=True) -> np.int:
    """ Loads saved model and optimizer states if exists
    @param net: Neural Network object
    @param optimizer: pytorch optimizer
    @param scheduler: pytorch scheduler
    @param args: AlphaZeroArgs
    @param iteration: current iteration
    @param new_optim_state:
    @return:
    """
    checkpoint_path = util.get_model_file_path(args.neural_net_name, iteration)
    start_epoch, checkpoint = 0, None
    if os.path.isfile(checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
    if checkpoint is not None:
        if len(checkpoint) == 1 or new_optim_state:
            net.load_state_dict(checkpoint['state_dict'])
            print("Loaded checkpoint model {checkpoint_path}")
        else:
            start_epoch = checkpoint['epoch']
            net.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            scheduler.load_state_dict(checkpoint['scheduler'])
            print(f"Loaded checkpoint model {checkpoint_path}, and optimizer, scheduler.")
    return start_epoch

@timer
def load_results(iteration):
    """ Loads saved results if exists """
    losses_path = util.get_losses_file(iteration)
    if os.path.isfile(losses_path):
        losses_per_epoch = util.pickle_load(losses_path)
        print("Loaded results buffer")
    else:
        losses_per_epoch = []
    return losses_per_epoch

@timer
def train(
        net: Connect4Network,
        dataset: np.ndarray,
        optimizer: torch.optim.Optimizer,
        scheduler: torch.optim.lr_scheduler.MultiStepLR,
        start_epoch : int,
        iteration: np.int,
        arguments: AlphaZeroArgs,
        cpu: int = 0):
    """
    Training function, optimizing the weights of our NeuralNetwork using state, policy and value of the datasets
    @param net: Neural Network
    @param dataset: Dataset generated by Self play
    @param optimizer: Pytorch optimizer
    @param scheduler: Pytorch scheduler
    @param start_epoch: start epoch
    @param iteration: current iteration
    @param arguments: AlphaZeroArgs
    @param cpu: CPU index
    """
    torch.manual_seed(cpu)
    cuda = torch.cuda.is_available()
    # Neural Net in TrainMode
    net.train()

    # Custom Alpha loss function
    criterion = AlphaLoss()

    # Initialize Training Set
    train_set = BoardData(dataset)
    train_loader = DataLoader(train_set, batch_size=arguments.batch_size, shuffle=True, num_workers=0, pin_memory=False)
    losses_per_epoch = load_results(iteration + 1)

    print("Starting training process...")
    if len(train_loader) > 10:
        update_rate = len(train_loader) // 10
    else:
        update_rate = 1
    print("Update step rate: %d" % update_rate)
    for epoch in range(start_epoch, arguments.num_epochs):
        total_loss = 0.0
        losses_per_batch = []
        for i, data in enumerate(train_loader, 0):
            # Training using State, policy, value generated by MCTS
            state, policy, value = data
            state, policy, value = state.float(), policy.float(), value.float()
            # CUDA check
            if cuda:
                state, policy, value = state.cuda(), policy.cuda(), value.cuda()
            policy_pred, value_pred = net(state)
            # AlphaLoss for calculation
            loss = criterion(value_pred[:, 0], value, policy_pred, policy)
            loss = loss / arguments.gradient_acc_steps
            loss.backward()
            clip_grad_norm_(net.parameters(), arguments.max_norm)
            if (epoch % arguments.gradient_acc_steps) == 0:
                # Using the
                optimizer.step()
                optimizer.zero_grad()

            # Add current loss to total
            total_loss += loss.item()
            if i % update_rate == (update_rate - 1):  # print every update_size-d mini-batches of size = batch_size
                losses_per_batch.append(arguments.gradient_acc_steps * total_loss / update_rate)
                print('[Iteration %d] Process ID: %d [Epoch: %d, %5d/ %d points] total loss per batch: %.3f' %
                      (iteration, os.getpid(), epoch + 1, (i + 1) * arguments.batch_size, len(train_set),
                       losses_per_batch[-1]))
                print("Policy (actual, predicted):", policy[0].argmax().item(), policy_pred[0].argmax().item())
                print("Policy data:", policy[0])
                print("Policy pred:", policy_pred[0])
                print("Value (actual, predicted):", value[0].item(), value_pred[0, 0].item())
                # print("Conv grad: %.7f" % net.conv.conv1.weight.grad.mean().item())
                # print("Res18 grad %.7f:" % net.res_18.conv1.weight.grad.mean().item())
                total_loss = 0.0

        scheduler.step()
        if len(losses_per_batch) >= 1:
            losses_per_epoch.append(sum(losses_per_batch) / len(losses_per_batch))
        if (epoch % 2) == 0:
            # Save trained Model
            util.pickle_save(util.get_losses_file(iteration + 1), losses_per_epoch)

            torch_dest = util.get_model_file_path(arguments.neural_net_name, iteration + 1)
            torch.save({
                'epoch': epoch + 1,
                'state_dict': net.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
            }, torch_dest)
        '''
        # Early stopping
        if len(losses_per_epoch) > 50:
            if abs(sum(losses_per_epoch[-4:-1])/3-sum(losses_per_epoch[-16:-13])/3) <= 0.00017:
                break
        '''

    print("Finished Training!")
    # Plotting Feature
    fig = plt.figure()
    ax = fig.add_subplot(222)
    ax.scatter([e for e in range(start_epoch, (len(losses_per_epoch) + start_epoch))], losses_per_epoch)
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Loss per batch")
    ax.set_title("Loss vs Epoch")

    util.create_model_directory()

    # Save Plot as PNG
    plt.savefig(os.path.join(
        util.get_model_directory(),
        f"Loss_vs_Epoch_iter_{iteration + 1}_{datetime.datetime.today().strftime('%Y-%m-%d')}.png")
    )
    plt.show()

@timer
def train_connect4(arguments: AlphaZeroArgs, iteration: int, new_optim_state: bool):
    """
    Function handling the trainings process
    @param arguments: AlphaZeroArgs
    @param iteration: current Iteration
    @param new_optim_state:
    """
    print("Loading training data...")
    data_path = util.get_datasets_dir(iteration)
    datasets = []
    for idx, file in enumerate(os.listdir(data_path)):
        filename = os.path.join(data_path, file)
        with open(filename, 'rb') as fo:
            datasets.extend(pickle.load(fo, encoding='bytes'))
    datasets = np.array(datasets)
    print("Loaded data from %s." % data_path)

    # train net
    net = Connect4Network()
    if torch.cuda.is_available():
        net.cuda()

    # Setup of training, using optimizer=Adam, scheduler = MultiStepLR
    optimizer = optim.Adam(net.parameters(), lr=arguments.lr, betas=(0.8, 0.999))
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 400], gamma=0.77)
    start_epoch = load_state(net, optimizer, scheduler, arguments, iteration, new_optim_state)

    train(net, datasets, optimizer, scheduler, start_epoch, iteration, arguments, 0)
